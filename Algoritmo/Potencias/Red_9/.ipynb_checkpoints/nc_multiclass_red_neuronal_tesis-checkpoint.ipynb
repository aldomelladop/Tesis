{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Nov 23 17:14:06 2019\n",
    "\n",
    "@author: aldo_mellado\n",
    "\"\"\"\n",
    "# ============================================================================\n",
    "# Importing the libraries\n",
    "# =============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pytictoc import TicToc\n",
    "from fixrows import fixrows\n",
    "from merge_csv import fusionar_csv\n",
    "from itertools import product\n",
    "# =============================================================================\n",
    "# Importing the dataset\n",
    "# =============================================================================\n",
    "\n",
    "a = list(product([10000],['s','n']))\n",
    "duration = 1 #segundos\n",
    "f1 = 440    #término de procesos simples\n",
    "f2 = 550    #Termino de grid search\n",
    "f3 = 650    #Termino de cross_validation\n",
    "f4 = 750    #Termino del codigo, paso al siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j = 10000\n",
      "son = s\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R1_00_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R1_00\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R1_01_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R1_01\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R1_02_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R1_02\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R2_10_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R2_10\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R2_11_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R2_11\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R2_12_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R2_12\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R3_20_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R3_20\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R3_21_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R3_21\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R3_22_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R3_22\n",
      "Archivos fusionados: ('Potencia_R1_00', 'Potencia_R1_01', 'Potencia_R1_02', 'Potencia_R2_10', 'Potencia_R2_11', 'Potencia_R2_12', 'Potencia_R3_20', 'Potencia_R3_21', 'Potencia_R3_22')\n",
      "S\n",
      "Comenzando Grid_search\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(a)):\n",
    "  \n",
    "    j = a[i][0]\n",
    "    son = a[i][1].capitalize()\n",
    "    print(f\"j = {a[i][0]}\\nson = {a[i][1]}\")\n",
    "\n",
    "    directory = os.getcwd()\n",
    "    \n",
    "    if os.path.isdir(os.getcwd() + '/{}_{}'.format(j,son)) == True:\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(os.getcwd() + '/{}_{}'.format(j,son))\n",
    "    \n",
    "    t = TicToc()\n",
    "\n",
    "    if son=='S':\n",
    "        df1 = fixrows('Potencia_r1_00').iloc[:j,:]\n",
    "        num_row = np.shape(df1)[0]\n",
    "        coords  = ['(0,0)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df1 = coords.join(df1, how='left')\n",
    "        df1.to_csv('Potencia_R1_00.csv')\n",
    "         \n",
    "        df2 = fixrows('Potencia_r1_01').iloc[:j,:]\n",
    "        num_row = np.shape(df2)[0]\n",
    "        coords  = ['(0,1)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df2 = coords.join(df2, how='left')\n",
    "        df2.to_csv('Potencia_R1_01.csv')\n",
    "\n",
    "        df3 = fixrows('Potencia_r1_02').iloc[:j,:]\n",
    "        num_row = np.shape(df3)[0]\n",
    "        coords  = ['(0,2)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df3 = coords.join(df3, how='left')\n",
    "        df3.to_csv('Potencia_R1_02.csv')\n",
    "\n",
    "        df4 = fixrows('Potencia_r2_10').iloc[:j,:]\n",
    "        num_row = np.shape(df4)[0]\n",
    "        coords  = ['(1,0)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df4 = coords.join(df4, how='left')\n",
    "        df4.to_csv('Potencia_R2_10.csv')\n",
    "\n",
    "        df5 = fixrows('Potencia_r2_11').iloc[:j,:]\n",
    "        num_row = np.shape(df5)[0]\n",
    "        coords  = ['(1,1)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df5 = coords.join(df5, how='left')\n",
    "        df5.to_csv('Potencia_R2_11.csv')\n",
    "\n",
    "        df6 = fixrows('Potencia_r2_12').iloc[:j,:]\n",
    "        num_row = np.shape(df6)[0]\n",
    "        coords  = ['(1,2)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df6 = coords.join(df6, how='left')\n",
    "        df6.to_csv('Potencia_R2_12.csv')\n",
    "\n",
    "        df7 = fixrows('Potencia_r3_20').iloc[:j,:]\n",
    "        num_row = np.shape(df7)[0]\n",
    "        coords  = ['(2,0)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df7 = coords.join(df7, how='left')\n",
    "        df7.to_csv('Potencia_R3_20.csv')\n",
    "\n",
    "        df8 = fixrows('Potencia_r3_21').iloc[:j,:]\n",
    "        num_row = np.shape(df8)[0]\n",
    "        coords  = ['(2,1)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df8 = coords.join(df8, how='left')\n",
    "        df8.to_csv('Potencia_R3_21.csv')\n",
    "\n",
    "        df9 = fixrows('Potencia_r3_22').iloc[:j,:]\n",
    "        num_row = np.shape(df9)[0]\n",
    "        coords  = ['(2,2)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df9 = coords.join(df9, how='left')\n",
    "        df9.to_csv('Potencia_R3_22.csv')\n",
    "          \n",
    "        # Fusionar archivos corregidos para obtener el archivo de potencias final\n",
    "        fusionar_csv('Potencia_R1_00','Potencia_R1_01','Potencia_R1_02','Potencia_R2_10','Potencia_R2_11',\n",
    "                 'Potencia_R2_12','Potencia_R3_20','Potencia_R3_21','Potencia_R3_22')\n",
    "         \n",
    "        df0 = fixrows('potencias_fusionado').iloc[:,1:]\n",
    "        os.system('rm Potencia_R*')\n",
    "        os.system('rm Potencia_r*_corregido.csv')\n",
    "        df0 = pd.read_csv('potencias_fusionado_corregido.csv').iloc[3:,1:]\n",
    "        os.system('play -nq -t alsa synth {} sine {}'.format(duration,f1))\n",
    "    else:\n",
    "        df0 = pd.read_csv('potencias_fusionado_corregido.csv').iloc[3:,1:]\n",
    "        os.system('play -nq -t alsa synth {} sine {}'.format(duration,f1))\n",
    "\n",
    "    X = df0.iloc[:,1:].values #variables Dependientes (Potencias)\n",
    "    y = df0.iloc[:,0].values #values Independientes (Posición)\n",
    "\n",
    "    from keras.utils import np_utils\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    y_encoded = encoder.transform(y)\n",
    "    y_encoded = np_utils.to_categorical(y_encoded)\n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    # =============================================================================\n",
    "    # Feature Scaling (Standarization)\n",
    "    # =============================================================================\n",
    "    if son == 'S':\n",
    "        print(son)\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "    else:\n",
    "        print(son)\n",
    "        # Feature Scaling (Normalization)\n",
    "        from sklearn import preprocessing\n",
    "        X_train = preprocessing.normalize(X_train)\n",
    "        X_test  = preprocessing.normalize(X_test)\n",
    "    # =============================================================================\n",
    "    # Buscar mejores parámetros\n",
    "    # =============================================================================\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    print(\"Comenzando Grid_search\\n\")\n",
    "    t.tic()\n",
    "\n",
    "    def build_classifier(optimizer):\n",
    "        classifier = Sequential()\n",
    "        classifier.add(Dense(units = np.shape(X_test)[1]+1, kernel_initializer = 'uniform', activation = 'relu', input_dim = np.shape(X_test)[1]))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "        \n",
    "        classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "        \n",
    "        classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))    \n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "        classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "        classifier.add(Dense(units = np.shape(y_test)[1], kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "        classifier.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        return classifier\n",
    "\n",
    "    classifier = KerasClassifier(build_fn = build_classifier)\n",
    "    parameters = {'batch_size': [16,32,48],'epochs': [15, 25, 35],'optimizer': ['adam', 'adamax','rmsprop']}\n",
    "\n",
    "    grid = GridSearchCV(estimator = classifier,param_grid = parameters,\n",
    "    #                           scoring = 'accuracy',\n",
    "                        cv = 3,n_jobs=-3)\n",
    "\n",
    "    grid_search_results = grid.fit(X_train, y_train)\n",
    "\n",
    "    best_parameters  = grid_search_results.best_params_\n",
    "    print(f\"best_parameters = {grid_search_results.best_params_}\")\n",
    "    print(f\"best_accuracy =   {grid_search_results.best_score_}\")\n",
    "    t.toc('Finalizado - Grid_search')\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration,f2))\n",
    "    t1 = t.tocvalue()\n",
    "\n",
    "    means = grid_search_results.cv_results_['mean_test_score']\n",
    "    stds = grid_search_results.cv_results_['std_test_score']\n",
    "    params = grid_search_results.cv_results_['params']\n",
    "\n",
    "    # =============================================================================\n",
    "    # Cross Validation\n",
    "    # =============================================================================\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    t.tic() \n",
    "    print(\"\\nEntrando en Cross Validation\\n\")\n",
    "\n",
    "    def build_classifier():\n",
    "        classifier = Sequential()\n",
    "        classifier.add(Dense(units = np.shape(X_test)[1]+1, kernel_initializer = 'uniform', activation = 'relu', input_dim = np.shape(X_test)[1]))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "        \n",
    "        classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "        \n",
    "        classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))    \n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "        classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "        classifier.add(Dense(units = np.shape(y_test)[1], kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "        classifier.compile(optimizer = best_parameters['optimizer'], loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        return classifier\n",
    "\n",
    "    classifier = KerasClassifier(build_fn = build_classifier, batch_size = best_parameters['batch_size'], epochs = best_parameters['epochs'])\n",
    "\n",
    "    accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5, n_jobs = -1)\n",
    "    ac = list(accuracies)\n",
    "    mean = accuracies.mean()\n",
    "    variance = accuracies.std()\n",
    "    t.toc('\\nTiempo Cross-Validation: ')\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration,f3))\n",
    "    time = t.tocvalue()\n",
    "\n",
    "    # =============================================================================\n",
    "    #     Distribución de probabilidad\n",
    "    # =============================================================================\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "        \n",
    "    print('>> Mean CV score is: ', round(np.mean(accuracies),3))\n",
    "    pltt = sns.distplot(pd.Series(accuracies,name='CV scores distribution'), color='r')\n",
    "    plt.savefig('CV_Accuracies_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    # =====================================================\n",
    "    #     Saving model\n",
    "    # =============================================================================\n",
    "\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = np.shape(X_test)[1]+1, kernel_initializer = 'uniform', activation = 'relu', input_dim = np.shape(X_test)[1]))\n",
    "    classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "    classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "    classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))    \n",
    "    classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "    classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))    \n",
    "    classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "    classifier.add(Dense(units = np.shape(y_test)[1], kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "    classifier.compile(optimizer = best_parameters['optimizer'], loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = classifier.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # save model and architecture to single file\n",
    "    classifier.save(directory + \"/{}_{}/model_{}.h5\".format(j,son,j))\n",
    "    print(\"Saved model to disk\\n\")\n",
    "\n",
    "    # =============================================================================\n",
    "    #     Escritura de archivo\n",
    "    # =============================================================================\n",
    "    print(f\"\\nnp.shape(X_test)[0] = {np.shape(X_test)[0]}\\n\")\n",
    "\n",
    "    outFileName= directory + \"/{}_{}/resultados_{}.txt\".format(j,son,j)\n",
    "    f = open(outFileName,\"w\")\n",
    "\n",
    "    f.write(\"El número de elementos usados es: {}\\n\".format(j)+\n",
    "        \"Los mejores parámetros son: {}\".format(best_parameters) +\n",
    "        \"\\nTiempo de GridSearchCV  = {}\".format(round(int(t1/60),2)) + \n",
    "        \"\\nTiempo red neuronal  =  {}\".format(round(int(time/60),2)) + \n",
    "        \"\\nLa media obtenida es: {}\".format(mean)+\n",
    "        \"\\nLa varianza obtenida es: {}\".format(variance) + '\\n'\n",
    "        )\n",
    "    for i in ac:\n",
    "        f.write(\"\\tac: \" + repr(round((i*100),2)) +\"%\" + '\\n')\n",
    "\n",
    "    for i,j in enumerate(zip(params,means,stds)):\n",
    "        f.write(\"\\nparams[{}] = {} --> means[{}] = {}\\n\".format(i,params[i],i,round(means[i],2)))\n",
    "    # =============================================================================\n",
    "    #                             Prediction\n",
    "    # =============================================================================\n",
    "    for i in range(1,15):\n",
    "        r = random.randint(0, np.shape(X_test)[0]-1)\n",
    "        y_pred = classifier.predict(np.array([X_test[r]]))\n",
    "        predictions = list(encoder.inverse_transform([np.argmax(y_pred, axis=None, out=None)]))\n",
    "        y_pred_prob = classifier.predict_proba(np.array([X_test[r]]))\n",
    "        f.write(\"\\nFor the vector: [\"+ repr(X_test[r])+ \"]\\t the predicted position is:\" +  repr(predictions) +  \"and its accuracy was:\" + repr(round(np.amax(y_pred_prob),2)))\n",
    "    f.close()\n",
    "\n",
    "    print(\"Archivo escrito\\n\")\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration,f1))\n",
    "    # =============================================================================\n",
    "    # Full multiclass report \n",
    "    # =============================================================================\n",
    "\n",
    "    from plot_history import * \n",
    "    from full_multiclass_report import * \n",
    "\n",
    "    history = classifier.fit(X_train, \n",
    "                    y_train,\n",
    "                    epochs = best_parameters['epochs'],\n",
    "                    batch_size = best_parameters['batch_size'],\n",
    "                    verbose=0,\n",
    "                    validation_data=(X_test,y_test))\n",
    "    plot_history(history)\n",
    "\n",
    "    full_multiclass_report(classifier,X_test,y_test,classes=['(0,0)','(0,1)','(0,2)','(1,0)','(1,1)','(1,2)','(2,0)','(2,1)','(2,2)'])\n",
    "\n",
    "    # =============================================================================\n",
    "    #     Move file to folder\n",
    "    # =============================================================================\n",
    "    mv = directory + '/{}_{}/'.format(j,son)\n",
    "    os.system('mv Confusion_matrix.png '+ mv+ 'Confusion_matrix.png')\n",
    "    os.system('mv CV_Accuracies_distribution.png '+ mv+ 'CV_Accuracies_distribution.png')\n",
    "    os.system('mv Loss.png '+ mv + 'Loss.png')\n",
    "    os.system('mv Accuracy.png '+ mv + 'Accuracy.png')\n",
    "    os.system('mv Classification_report.csv ' + mv + 'Classification_report.csv')\n",
    "    os.system('mv model.json ' + mv + 'model_{}_{}.json'.format(j,son))\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration,f4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
