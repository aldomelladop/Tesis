{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Nov 23 17:14:06 2019\n",
    "\n",
    "@author: aldo_mellado\n",
    "\"\"\"\n",
    "# ============================================================================\n",
    "# Importing the libraries\n",
    "# =============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pytictoc import TicToc\n",
    "from fixrows import fixrows\n",
    "from merge_csv import fusionar_csv\n",
    "from itertools import product\n",
    "# =============================================================================\n",
    "# Importing the dataset\n",
    "# =============================================================================\n",
    "\n",
    "a = list(product([10000],['s','n']))\n",
    "duration = 1 #segundos\n",
    "f1 = 440    #t√©rmino de procesos simples\n",
    "f2 = 550    #Termino de grid search\n",
    "f3 = 650    #Termino de cross_validation\n",
    "f4 = 750    #Termino del codigo, paso al siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j = 10000\n",
      "son = s\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R1_00_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R1_00\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R1_01_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R1_01\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R1_02_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R1_02\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R2_10_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R2_10\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R2_11_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R2_11\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R2_12_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R2_12\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R3_20_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R3_20\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R3_21_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R3_21\n",
      "Copiado... /home/aldo_mellado/Tesis/Algoritmo/Potencias/Red_9/Potencia_R3_22_copia.csv\n",
      "Archivo copiado\n",
      "Archivo redimensionado\n",
      "file : Potencia_R3_22\n",
      "Archivos fusionados: ('Potencia_R1_00', 'Potencia_R1_01', 'Potencia_R1_02', 'Potencia_R2_10', 'Potencia_R2_11', 'Potencia_R2_12', 'Potencia_R3_20', 'Potencia_R3_21', 'Potencia_R3_22')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "Comenzando Grid_search\n",
      "\n",
      "WARNING:tensorflow:From /home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aldo_mellado/anaconda3/envs/env_full/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/15\n",
      "48002/48002 [==============================] - 13s 263us/step - loss: 1.1561 - acc: 0.4908\n",
      "Epoch 2/15\n",
      "48002/48002 [==============================] - 11s 235us/step - loss: 0.6457 - acc: 0.7576\n",
      "Epoch 3/15\n",
      "48002/48002 [==============================] - 11s 234us/step - loss: 0.5177 - acc: 0.8030\n",
      "Epoch 4/15\n",
      "48002/48002 [==============================] - 11s 236us/step - loss: 0.4615 - acc: 0.8212\n",
      "Epoch 5/15\n",
      "48002/48002 [==============================] - 11s 235us/step - loss: 0.4138 - acc: 0.8351\n",
      "Epoch 6/15\n",
      "48002/48002 [==============================] - 11s 234us/step - loss: 0.3851 - acc: 0.8431\n",
      "Epoch 7/15\n",
      "48002/48002 [==============================] - 11s 235us/step - loss: 0.3664 - acc: 0.8474\n",
      "Epoch 8/15\n",
      "48002/48002 [==============================] - 11s 233us/step - loss: 0.3599 - acc: 0.8508\n",
      "Epoch 9/15\n",
      "48002/48002 [==============================] - 11s 234us/step - loss: 0.3378 - acc: 0.8698\n",
      "Epoch 10/15\n",
      "48002/48002 [==============================] - 11s 235us/step - loss: 0.2924 - acc: 0.9245\n",
      "Epoch 11/15\n",
      "48002/48002 [==============================] - 11s 236us/step - loss: 0.2570 - acc: 0.9407\n",
      "Epoch 12/15\n",
      "48002/48002 [==============================] - 11s 234us/step - loss: 0.2409 - acc: 0.9451\n",
      "Epoch 13/15\n",
      "48002/48002 [==============================] - 11s 235us/step - loss: 0.2286 - acc: 0.9487\n",
      "Epoch 14/15\n",
      "48002/48002 [==============================] - 11s 234us/step - loss: 0.2290 - acc: 0.9475\n",
      "Epoch 15/15\n",
      "48002/48002 [==============================] - 12s 257us/step - loss: 0.2201 - acc: 0.9505\n",
      "24002/24002 [==============================] - 2s 101us/step\n",
      "Epoch 1/15\n",
      "48003/48003 [==============================] - 13s 272us/step - loss: 0.9176 - acc: 0.6090\n",
      "Epoch 2/15\n",
      "48003/48003 [==============================] - 11s 238us/step - loss: 0.5461 - acc: 0.7637\n",
      "Epoch 3/15\n",
      "48003/48003 [==============================] - 11s 238us/step - loss: 0.4531 - acc: 0.8163\n",
      "Epoch 4/15\n",
      "48003/48003 [==============================] - 12s 249us/step - loss: 0.3574 - acc: 0.8933\n",
      "Epoch 5/15\n",
      "48003/48003 [==============================] - 11s 237us/step - loss: 0.2753 - acc: 0.9341\n",
      "Epoch 6/15\n",
      "48003/48003 [==============================] - 12s 240us/step - loss: 0.2386 - acc: 0.9459\n",
      "Epoch 7/15\n",
      "48003/48003 [==============================] - 11s 238us/step - loss: 0.2211 - acc: 0.9512\n",
      "Epoch 8/15\n",
      "48003/48003 [==============================] - 12s 240us/step - loss: 0.2070 - acc: 0.9538\n",
      "Epoch 9/15\n",
      "48003/48003 [==============================] - 11s 238us/step - loss: 0.1970 - acc: 0.9558\n",
      "Epoch 10/15\n",
      "48003/48003 [==============================] - 11s 238us/step - loss: 0.1865 - acc: 0.9591\n",
      "Epoch 11/15\n",
      "48003/48003 [==============================] - 12s 241us/step - loss: 0.1783 - acc: 0.9617\n",
      "Epoch 12/15\n",
      "48003/48003 [==============================] - 11s 237us/step - loss: 0.1736 - acc: 0.9636\n",
      "Epoch 13/15\n",
      "48003/48003 [==============================] - 11s 237us/step - loss: 0.1784 - acc: 0.9628\n",
      "Epoch 14/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.1684 - acc: 0.9643\n",
      "Epoch 15/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.1713 - acc: 0.9645\n",
      "24001/24001 [==============================] - 3s 104us/step\n",
      "Epoch 1/15\n",
      "48003/48003 [==============================] - 13s 276us/step - loss: 1.0107 - acc: 0.5607\n",
      "Epoch 2/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.5451 - acc: 0.7903\n",
      "Epoch 3/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.4004 - acc: 0.8904\n",
      "Epoch 4/15\n",
      "48003/48003 [==============================] - 12s 240us/step - loss: 0.3180 - acc: 0.9207\n",
      "Epoch 5/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.2821 - acc: 0.9295\n",
      "Epoch 6/15\n",
      "48003/48003 [==============================] - 12s 241us/step - loss: 0.2677 - acc: 0.9349\n",
      "Epoch 7/15\n",
      "48003/48003 [==============================] - 12s 240us/step - loss: 0.2503 - acc: 0.9375\n",
      "Epoch 8/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.2385 - acc: 0.9408\n",
      "Epoch 9/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.2232 - acc: 0.9449\n",
      "Epoch 10/15\n",
      "48003/48003 [==============================] - 11s 238us/step - loss: 0.2262 - acc: 0.9431\n",
      "Epoch 11/15\n",
      "48003/48003 [==============================] - 13s 263us/step - loss: 0.2086 - acc: 0.9469\n",
      "Epoch 12/15\n",
      "48003/48003 [==============================] - 12s 241us/step - loss: 0.2119 - acc: 0.9474\n",
      "Epoch 13/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.2073 - acc: 0.9494\n",
      "Epoch 14/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.1936 - acc: 0.9527\n",
      "Epoch 15/15\n",
      "48003/48003 [==============================] - 12s 252us/step - loss: 0.1985 - acc: 0.9522\n",
      "24001/24001 [==============================] - 3s 109us/step\n",
      "Epoch 1/15\n",
      "48002/48002 [==============================] - 13s 273us/step - loss: 1.1183 - acc: 0.4985\n",
      "Epoch 2/15\n",
      "48002/48002 [==============================] - 11s 239us/step - loss: 0.7153 - acc: 0.6995\n",
      "Epoch 3/15\n",
      "48002/48002 [==============================] - 11s 239us/step - loss: 0.5586 - acc: 0.7792\n",
      "Epoch 4/15\n",
      "48002/48002 [==============================] - 11s 238us/step - loss: 0.4578 - acc: 0.8186\n",
      "Epoch 5/15\n",
      "48002/48002 [==============================] - 11s 239us/step - loss: 0.3971 - acc: 0.8506\n",
      "Epoch 6/15\n",
      "48002/48002 [==============================] - 12s 240us/step - loss: 0.3475 - acc: 0.8853\n",
      "Epoch 7/15\n",
      "48002/48002 [==============================] - 11s 238us/step - loss: 0.2822 - acc: 0.9151\n",
      "Epoch 8/15\n",
      "48002/48002 [==============================] - 11s 238us/step - loss: 0.2608 - acc: 0.9220\n",
      "Epoch 9/15\n",
      "48002/48002 [==============================] - 11s 238us/step - loss: 0.2361 - acc: 0.9306\n",
      "Epoch 10/15\n",
      "48002/48002 [==============================] - 11s 238us/step - loss: 0.2264 - acc: 0.9342\n",
      "Epoch 11/15\n",
      "48002/48002 [==============================] - 11s 239us/step - loss: 0.2142 - acc: 0.9377\n",
      "Epoch 12/15\n",
      "48002/48002 [==============================] - 11s 239us/step - loss: 0.2089 - acc: 0.9409\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48002/48002 [==============================] - 12s 240us/step - loss: 0.2065 - acc: 0.9415\n",
      "Epoch 14/15\n",
      "48002/48002 [==============================] - 11s 238us/step - loss: 0.1980 - acc: 0.9440\n",
      "Epoch 15/15\n",
      "48002/48002 [==============================] - 11s 239us/step - loss: 0.1925 - acc: 0.9451\n",
      "24002/24002 [==============================] - 3s 111us/step\n",
      "Epoch 1/15\n",
      "48003/48003 [==============================] - 13s 276us/step - loss: 1.1745 - acc: 0.4287\n",
      "Epoch 2/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.7975 - acc: 0.6502\n",
      "Epoch 3/15\n",
      "48003/48003 [==============================] - 12s 240us/step - loss: 0.6192 - acc: 0.7471\n",
      "Epoch 4/15\n",
      "48003/48003 [==============================] - 12s 240us/step - loss: 0.5268 - acc: 0.8017\n",
      "Epoch 5/15\n",
      "48003/48003 [==============================] - 11s 240us/step - loss: 0.4504 - acc: 0.8371\n",
      "Epoch 6/15\n",
      "48003/48003 [==============================] - 12s 242us/step - loss: 0.4001 - acc: 0.8582\n",
      "Epoch 7/15\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 0.3694 - acc: 0.8735\n",
      "Epoch 8/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.3449 - acc: 0.8888\n",
      "Epoch 9/15\n",
      "48003/48003 [==============================] - 12s 240us/step - loss: 0.3158 - acc: 0.9075\n",
      "Epoch 10/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.2826 - acc: 0.9224\n",
      "Epoch 11/15\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.2602 - acc: 0.9277\n",
      "Epoch 12/15\n",
      "48003/48003 [==============================] - 12s 240us/step - loss: 0.2514 - acc: 0.9334\n",
      "Epoch 13/15\n",
      "48003/48003 [==============================] - 12s 240us/step - loss: 0.2414 - acc: 0.9355\n",
      "Epoch 14/15\n",
      "48003/48003 [==============================] - 12s 240us/step - loss: 0.2296 - acc: 0.9395\n",
      "Epoch 15/15\n",
      "48003/48003 [==============================] - 11s 239us/step - loss: 0.2101 - acc: 0.9434\n",
      "24001/24001 [==============================] - 3s 116us/step\n",
      "Epoch 1/15\n",
      "48003/48003 [==============================] - 14s 283us/step - loss: 1.2422 - acc: 0.4495\n",
      "Epoch 2/15\n",
      "48003/48003 [==============================] - 12s 243us/step - loss: 0.8324 - acc: 0.6189\n",
      "Epoch 3/15\n",
      "48003/48003 [==============================] - 12s 241us/step - loss: 0.7320 - acc: 0.6564\n",
      "Epoch 4/15\n",
      "48003/48003 [==============================] - 12s 242us/step - loss: 0.6697 - acc: 0.7190\n",
      "Epoch 5/15\n",
      "48003/48003 [==============================] - 12s 241us/step - loss: 0.5768 - acc: 0.7943\n",
      "Epoch 6/15\n",
      "48003/48003 [==============================] - 12s 242us/step - loss: 0.5013 - acc: 0.8365\n",
      "Epoch 7/15\n",
      "48003/48003 [==============================] - 12s 242us/step - loss: 0.4547 - acc: 0.8609\n",
      "Epoch 8/15\n",
      "48003/48003 [==============================] - 12s 242us/step - loss: 0.3981 - acc: 0.8852\n",
      "Epoch 9/15\n",
      "48003/48003 [==============================] - 12s 242us/step - loss: 0.3549 - acc: 0.9026\n",
      "Epoch 10/15\n",
      "48003/48003 [==============================] - 12s 241us/step - loss: 0.3246 - acc: 0.9138\n",
      "Epoch 11/15\n",
      "48003/48003 [==============================] - 12s 241us/step - loss: 0.3078 - acc: 0.9196\n",
      "Epoch 12/15\n",
      "48003/48003 [==============================] - 12s 244us/step - loss: 0.2921 - acc: 0.9236\n",
      "Epoch 13/15\n",
      "48003/48003 [==============================] - 12s 241us/step - loss: 0.2754 - acc: 0.9290\n",
      "Epoch 14/15\n",
      "48003/48003 [==============================] - 12s 241us/step - loss: 0.2571 - acc: 0.9340\n",
      "Epoch 15/15\n",
      "48003/48003 [==============================] - 12s 242us/step - loss: 0.2452 - acc: 0.9376\n",
      "24001/24001 [==============================] - 3s 118us/step\n",
      "Epoch 1/15\n",
      "48002/48002 [==============================] - 14s 283us/step - loss: 1.0777 - acc: 0.5439\n",
      "Epoch 2/15\n",
      "48002/48002 [==============================] - 12s 248us/step - loss: 0.9882 - acc: 0.7825\n",
      "Epoch 3/15\n",
      "48002/48002 [==============================] - 13s 261us/step - loss: 2.6095 - acc: 0.7718\n",
      "Epoch 4/15\n",
      "48002/48002 [==============================] - 12s 244us/step - loss: 2.8124 - acc: 0.7759\n",
      "Epoch 5/15\n",
      "48002/48002 [==============================] - 12s 246us/step - loss: 2.9558 - acc: 0.7684\n",
      "Epoch 6/15\n",
      "48002/48002 [==============================] - 12s 257us/step - loss: 3.1503 - acc: 0.7627\n",
      "Epoch 7/15\n",
      "48002/48002 [==============================] - 12s 245us/step - loss: 4.0350 - acc: 0.7122\n",
      "Epoch 8/15\n",
      "48002/48002 [==============================] - 12s 243us/step - loss: 4.9250 - acc: 0.6621\n",
      "Epoch 9/15\n",
      "48002/48002 [==============================] - 12s 244us/step - loss: 5.2988 - acc: 0.6419\n",
      "Epoch 10/15\n",
      "48002/48002 [==============================] - 12s 244us/step - loss: 5.3390 - acc: 0.6415\n",
      "Epoch 11/15\n",
      "48002/48002 [==============================] - 12s 245us/step - loss: 5.2174 - acc: 0.6503\n",
      "Epoch 12/15\n",
      "48002/48002 [==============================] - 12s 245us/step - loss: 5.1751 - acc: 0.6538\n",
      "Epoch 13/15\n",
      "48002/48002 [==============================] - 12s 244us/step - loss: 5.5220 - acc: 0.6353\n",
      "Epoch 14/15\n",
      "48002/48002 [==============================] - 12s 244us/step - loss: 5.3186 - acc: 0.6457\n",
      "Epoch 15/15\n",
      "48002/48002 [==============================] - 12s 244us/step - loss: 5.4767 - acc: 0.6370\n",
      "24002/24002 [==============================] - 3s 122us/step\n",
      "Epoch 1/15\n",
      "48003/48003 [==============================] - 14s 286us/step - loss: 1.1335 - acc: 0.5124\n",
      "Epoch 2/15\n",
      "48003/48003 [==============================] - 12s 245us/step - loss: 4.1544 - acc: 0.6233\n",
      "Epoch 3/15\n",
      "48003/48003 [==============================] - 12s 246us/step - loss: 5.8134 - acc: 0.6095\n",
      "Epoch 4/15\n",
      "48003/48003 [==============================] - 12s 246us/step - loss: 6.5451 - acc: 0.5717\n",
      "Epoch 5/15\n",
      "48003/48003 [==============================] - 12s 248us/step - loss: 6.7439 - acc: 0.5601\n",
      "Epoch 6/15\n",
      "48003/48003 [==============================] - 12s 249us/step - loss: 7.2604 - acc: 0.5289\n",
      "Epoch 7/15\n",
      "48003/48003 [==============================] - 12s 247us/step - loss: 7.7127 - acc: 0.4979\n",
      "Epoch 8/15\n",
      "48003/48003 [==============================] - 12s 248us/step - loss: 8.7514 - acc: 0.4416\n",
      "Epoch 9/15\n",
      "48003/48003 [==============================] - 12s 247us/step - loss: 9.4532 - acc: 0.4051\n",
      "Epoch 10/15\n",
      "48003/48003 [==============================] - 12s 246us/step - loss: 10.0191 - acc: 0.3719\n",
      "Epoch 11/15\n",
      "48003/48003 [==============================] - 12s 248us/step - loss: 11.0750 - acc: 0.3069\n",
      "Epoch 12/15\n",
      "48003/48003 [==============================] - 12s 245us/step - loss: 10.9900 - acc: 0.3134\n",
      "Epoch 13/15\n",
      "48003/48003 [==============================] - 12s 258us/step - loss: 10.9496 - acc: 0.3159\n",
      "Epoch 14/15\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 11.1069 - acc: 0.3062\n",
      "Epoch 15/15\n",
      "48003/48003 [==============================] - 12s 246us/step - loss: 11.5283 - acc: 0.2817\n",
      "24001/24001 [==============================] - 3s 127us/step\n",
      "Epoch 1/15\n",
      "48003/48003 [==============================] - 14s 300us/step - loss: 2.1277 - acc: 0.4223\n",
      "Epoch 2/15\n",
      "48003/48003 [==============================] - 12s 247us/step - loss: 4.5253 - acc: 0.4754\n",
      "Epoch 3/15\n",
      "48003/48003 [==============================] - 12s 247us/step - loss: 6.2838 - acc: 0.4106\n",
      "Epoch 4/15\n",
      "48003/48003 [==============================] - 12s 246us/step - loss: 8.3792 - acc: 0.3985\n",
      "Epoch 5/15\n",
      "48003/48003 [==============================] - 12s 248us/step - loss: 8.6667 - acc: 0.3900\n",
      "Epoch 6/15\n",
      "48003/48003 [==============================] - 12s 247us/step - loss: 8.7339 - acc: 0.3822\n",
      "Epoch 7/15\n",
      "48003/48003 [==============================] - 12s 246us/step - loss: 8.9344 - acc: 0.3708\n",
      "Epoch 8/15\n",
      "48003/48003 [==============================] - 12s 247us/step - loss: 10.2220 - acc: 0.2970\n",
      "Epoch 9/15\n",
      "48003/48003 [==============================] - 12s 248us/step - loss: 11.1073 - acc: 0.2610\n",
      "Epoch 10/15\n",
      "48003/48003 [==============================] - 12s 245us/step - loss: 10.6881 - acc: 0.2887\n",
      "Epoch 11/15\n",
      "48003/48003 [==============================] - 12s 250us/step - loss: 10.9462 - acc: 0.2742\n",
      "Epoch 12/15\n",
      "48003/48003 [==============================] - 12s 247us/step - loss: 11.2422 - acc: 0.2563\n",
      "Epoch 13/15\n",
      "48003/48003 [==============================] - 12s 246us/step - loss: 12.0462 - acc: 0.2088\n",
      "Epoch 14/15\n",
      "48003/48003 [==============================] - 12s 246us/step - loss: 12.5136 - acc: 0.1928\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48003/48003 [==============================] - 12s 245us/step - loss: 13.0427 - acc: 0.1806\n",
      "24001/24001 [==============================] - 3s 132us/step\n",
      "Epoch 1/25\n",
      "48002/48002 [==============================] - 15s 304us/step - loss: 1.1097 - acc: 0.4979\n",
      "Epoch 2/25\n",
      "48002/48002 [==============================] - 12s 250us/step - loss: 0.7348 - acc: 0.6957\n",
      "Epoch 3/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.5484 - acc: 0.8345\n",
      "Epoch 4/25\n",
      "48002/48002 [==============================] - 12s 250us/step - loss: 0.4190 - acc: 0.8825\n",
      "Epoch 5/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.3553 - acc: 0.8944\n",
      "Epoch 6/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.3301 - acc: 0.9013\n",
      "Epoch 7/25\n",
      "48002/48002 [==============================] - 12s 250us/step - loss: 0.3040 - acc: 0.9087\n",
      "Epoch 8/25\n",
      "48002/48002 [==============================] - 13s 262us/step - loss: 0.2994 - acc: 0.9113\n",
      "Epoch 9/25\n",
      "48002/48002 [==============================] - 12s 260us/step - loss: 0.2707 - acc: 0.9226\n",
      "Epoch 10/25\n",
      "48002/48002 [==============================] - 12s 252us/step - loss: 0.2564 - acc: 0.9289\n",
      "Epoch 11/25\n",
      "48002/48002 [==============================] - 13s 261us/step - loss: 0.2473 - acc: 0.9330\n",
      "Epoch 12/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.2430 - acc: 0.9346\n",
      "Epoch 13/25\n",
      "48002/48002 [==============================] - 12s 250us/step - loss: 0.2424 - acc: 0.9340\n",
      "Epoch 14/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.2467 - acc: 0.9350\n",
      "Epoch 15/25\n",
      "48002/48002 [==============================] - 12s 252us/step - loss: 0.2349 - acc: 0.9367\n",
      "Epoch 16/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.2326 - acc: 0.9386\n",
      "Epoch 17/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.2332 - acc: 0.9371\n",
      "Epoch 18/25\n",
      "48002/48002 [==============================] - 12s 250us/step - loss: 0.2343 - acc: 0.9382\n",
      "Epoch 19/25\n",
      "48002/48002 [==============================] - 12s 250us/step - loss: 0.2236 - acc: 0.9403\n",
      "Epoch 20/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.2242 - acc: 0.9396\n",
      "Epoch 21/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.2206 - acc: 0.9413\n",
      "Epoch 22/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.2241 - acc: 0.9404\n",
      "Epoch 23/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.2213 - acc: 0.9412\n",
      "Epoch 24/25\n",
      "48002/48002 [==============================] - 12s 251us/step - loss: 0.2206 - acc: 0.9406\n",
      "Epoch 25/25\n",
      "48002/48002 [==============================] - 12s 252us/step - loss: 0.2217 - acc: 0.9409\n",
      "24002/24002 [==============================] - 3s 133us/step\n",
      "Epoch 1/25\n",
      "48003/48003 [==============================] - 15s 309us/step - loss: 1.0184 - acc: 0.5979\n",
      "Epoch 2/25\n",
      "48003/48003 [==============================] - 12s 253us/step - loss: 0.5396 - acc: 0.8124\n",
      "Epoch 3/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.3734 - acc: 0.8891\n",
      "Epoch 4/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.3013 - acc: 0.9124\n",
      "Epoch 5/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.2540 - acc: 0.9291\n",
      "Epoch 6/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.2276 - acc: 0.9391\n",
      "Epoch 7/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.2088 - acc: 0.9440\n",
      "Epoch 8/25\n",
      "48003/48003 [==============================] - 13s 266us/step - loss: 0.1997 - acc: 0.9484\n",
      "Epoch 9/25\n",
      "48003/48003 [==============================] - 13s 265us/step - loss: 0.1910 - acc: 0.9500\n",
      "Epoch 10/25\n",
      "48003/48003 [==============================] - 13s 266us/step - loss: 0.1845 - acc: 0.9521\n",
      "Epoch 11/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1803 - acc: 0.9538\n",
      "Epoch 12/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.1847 - acc: 0.9530\n",
      "Epoch 13/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.1663 - acc: 0.9580\n",
      "Epoch 14/25\n",
      "48003/48003 [==============================] - 12s 259us/step - loss: 0.1742 - acc: 0.9578\n",
      "Epoch 15/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.1639 - acc: 0.9598\n",
      "Epoch 16/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1612 - acc: 0.9605\n",
      "Epoch 17/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.1645 - acc: 0.9608\n",
      "Epoch 18/25\n",
      "48003/48003 [==============================] - 12s 253us/step - loss: 0.1623 - acc: 0.9610\n",
      "Epoch 19/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.1546 - acc: 0.9643\n",
      "Epoch 20/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.1483 - acc: 0.9647\n",
      "Epoch 21/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1432 - acc: 0.9664\n",
      "Epoch 22/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1458 - acc: 0.9665\n",
      "Epoch 23/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.1442 - acc: 0.9677\n",
      "Epoch 24/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.1327 - acc: 0.9698\n",
      "Epoch 25/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1327 - acc: 0.9690\n",
      "24001/24001 [==============================] - 3s 137us/step\n",
      "Epoch 1/25\n",
      "48003/48003 [==============================] - 15s 311us/step - loss: 1.1019 - acc: 0.5452\n",
      "Epoch 2/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.6380 - acc: 0.7992\n",
      "Epoch 3/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.4603 - acc: 0.8676\n",
      "Epoch 4/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.3752 - acc: 0.8994\n",
      "Epoch 5/25\n",
      "48003/48003 [==============================] - 12s 258us/step - loss: 0.3225 - acc: 0.9149\n",
      "Epoch 6/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.2718 - acc: 0.9255\n",
      "Epoch 7/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.2497 - acc: 0.9323\n",
      "Epoch 8/25\n",
      "48003/48003 [==============================] - 13s 278us/step - loss: 0.2401 - acc: 0.9381\n",
      "Epoch 9/25\n",
      "48003/48003 [==============================] - 12s 260us/step - loss: 0.2259 - acc: 0.9407\n",
      "Epoch 10/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 0.2150 - acc: 0.9436\n",
      "Epoch 11/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.2060 - acc: 0.9449\n",
      "Epoch 12/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1985 - acc: 0.9489\n",
      "Epoch 13/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.2002 - acc: 0.9489\n",
      "Epoch 14/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.1979 - acc: 0.9502\n",
      "Epoch 15/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.1883 - acc: 0.9534\n",
      "Epoch 16/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1885 - acc: 0.9528\n",
      "Epoch 17/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1907 - acc: 0.9526\n",
      "Epoch 18/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.1824 - acc: 0.9539\n",
      "Epoch 19/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.1749 - acc: 0.9571\n",
      "Epoch 20/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.1744 - acc: 0.9572\n",
      "Epoch 21/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1720 - acc: 0.9573\n",
      "Epoch 22/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1726 - acc: 0.9572\n",
      "Epoch 23/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1695 - acc: 0.9574\n",
      "Epoch 24/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1701 - acc: 0.9571\n",
      "Epoch 25/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.1626 - acc: 0.9591\n",
      "24001/24001 [==============================] - 3s 142us/step\n",
      "Epoch 1/25\n",
      "48002/48002 [==============================] - 15s 309us/step - loss: 1.2508 - acc: 0.4090\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48002/48002 [==============================] - 12s 254us/step - loss: 0.8516 - acc: 0.6133\n",
      "Epoch 3/25\n",
      "48002/48002 [==============================] - 12s 252us/step - loss: 0.6973 - acc: 0.6814\n",
      "Epoch 4/25\n",
      "48002/48002 [==============================] - 12s 254us/step - loss: 0.6135 - acc: 0.7115\n",
      "Epoch 5/25\n",
      "48002/48002 [==============================] - 12s 253us/step - loss: 0.5752 - acc: 0.7385\n",
      "Epoch 6/25\n",
      "48002/48002 [==============================] - 12s 253us/step - loss: 0.5172 - acc: 0.7785\n",
      "Epoch 7/25\n",
      "48002/48002 [==============================] - 12s 258us/step - loss: 0.4680 - acc: 0.8148\n",
      "Epoch 8/25\n",
      "48002/48002 [==============================] - 13s 272us/step - loss: 0.4235 - acc: 0.8411\n",
      "Epoch 9/25\n",
      "48002/48002 [==============================] - 13s 264us/step - loss: 0.3729 - acc: 0.8819\n",
      "Epoch 10/25\n",
      "48002/48002 [==============================] - 12s 253us/step - loss: 0.3377 - acc: 0.9072\n",
      "Epoch 11/25\n",
      "48002/48002 [==============================] - 12s 253us/step - loss: 0.3069 - acc: 0.9189\n",
      "Epoch 12/25\n",
      "48002/48002 [==============================] - 12s 256us/step - loss: 0.2925 - acc: 0.9233\n",
      "Epoch 13/25\n",
      "48002/48002 [==============================] - 12s 254us/step - loss: 0.2781 - acc: 0.9306\n",
      "Epoch 14/25\n",
      "48002/48002 [==============================] - 12s 255us/step - loss: 0.2637 - acc: 0.9341\n",
      "Epoch 15/25\n",
      "48002/48002 [==============================] - 12s 254us/step - loss: 0.2504 - acc: 0.9391\n",
      "Epoch 16/25\n",
      "48002/48002 [==============================] - 12s 253us/step - loss: 0.2370 - acc: 0.9422\n",
      "Epoch 17/25\n",
      "48002/48002 [==============================] - 12s 254us/step - loss: 0.2289 - acc: 0.9452\n",
      "Epoch 18/25\n",
      "48002/48002 [==============================] - 12s 253us/step - loss: 0.2193 - acc: 0.9469\n",
      "Epoch 19/25\n",
      "48002/48002 [==============================] - 12s 253us/step - loss: 0.2108 - acc: 0.9503\n",
      "Epoch 20/25\n",
      "48002/48002 [==============================] - 12s 253us/step - loss: 0.2005 - acc: 0.9524\n",
      "Epoch 21/25\n",
      "48002/48002 [==============================] - 12s 254us/step - loss: 0.1914 - acc: 0.9548\n",
      "Epoch 22/25\n",
      "48002/48002 [==============================] - 12s 254us/step - loss: 0.1815 - acc: 0.9573\n",
      "Epoch 23/25\n",
      "48002/48002 [==============================] - 12s 254us/step - loss: 0.1761 - acc: 0.9588\n",
      "Epoch 24/25\n",
      "48002/48002 [==============================] - 12s 253us/step - loss: 0.1725 - acc: 0.9603\n",
      "Epoch 25/25\n",
      "48002/48002 [==============================] - 12s 253us/step - loss: 0.1696 - acc: 0.9602\n",
      "24002/24002 [==============================] - 3s 146us/step\n",
      "Epoch 1/25\n",
      "48003/48003 [==============================] - 15s 317us/step - loss: 1.4245 - acc: 0.3396\n",
      "Epoch 2/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 1.0504 - acc: 0.5049\n",
      "Epoch 3/25\n",
      "48003/48003 [==============================] - 12s 253us/step - loss: 0.9151 - acc: 0.5844\n",
      "Epoch 4/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.8300 - acc: 0.6325\n",
      "Epoch 5/25\n",
      "48003/48003 [==============================] - 12s 253us/step - loss: 0.6898 - acc: 0.7161\n",
      "Epoch 6/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.6272 - acc: 0.7382\n",
      "Epoch 7/25\n",
      "48003/48003 [==============================] - 13s 264us/step - loss: 0.5789 - acc: 0.7556\n",
      "Epoch 8/25\n",
      "48003/48003 [==============================] - 13s 276us/step - loss: 0.5373 - acc: 0.7744\n",
      "Epoch 9/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.4939 - acc: 0.7901\n",
      "Epoch 10/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.4726 - acc: 0.7996\n",
      "Epoch 11/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.4481 - acc: 0.8132\n",
      "Epoch 12/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.4177 - acc: 0.8292\n",
      "Epoch 13/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.3935 - acc: 0.8381\n",
      "Epoch 14/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.3862 - acc: 0.8402\n",
      "Epoch 15/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.3790 - acc: 0.8422\n",
      "Epoch 16/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.3665 - acc: 0.8482\n",
      "Epoch 17/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.3677 - acc: 0.8492\n",
      "Epoch 18/25\n",
      "48003/48003 [==============================] - 12s 253us/step - loss: 0.3580 - acc: 0.8512\n",
      "Epoch 19/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.3563 - acc: 0.8514\n",
      "Epoch 20/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.3532 - acc: 0.8516\n",
      "Epoch 21/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.3482 - acc: 0.8521\n",
      "Epoch 22/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.3492 - acc: 0.8538\n",
      "Epoch 23/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.3411 - acc: 0.8550\n",
      "Epoch 24/25\n",
      "48003/48003 [==============================] - 12s 254us/step - loss: 0.3343 - acc: 0.8578\n",
      "Epoch 25/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.3327 - acc: 0.8594\n",
      "24001/24001 [==============================] - 4s 150us/step\n",
      "Epoch 1/25\n",
      "48003/48003 [==============================] - 15s 318us/step - loss: 1.3481 - acc: 0.4000\n",
      "Epoch 2/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.9122 - acc: 0.6498\n",
      "Epoch 3/25\n",
      "48003/48003 [==============================] - 12s 255us/step - loss: 0.7089 - acc: 0.7436\n",
      "Epoch 4/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.5873 - acc: 0.7883\n",
      "Epoch 5/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.5158 - acc: 0.8309\n",
      "Epoch 6/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.4413 - acc: 0.8706\n",
      "Epoch 7/25\n",
      "48003/48003 [==============================] - 13s 280us/step - loss: 0.3989 - acc: 0.8891\n",
      "Epoch 8/25\n",
      "48003/48003 [==============================] - 12s 259us/step - loss: 0.3478 - acc: 0.9038\n",
      "Epoch 9/25\n",
      "48003/48003 [==============================] - 12s 259us/step - loss: 0.3172 - acc: 0.9116\n",
      "Epoch 10/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.2961 - acc: 0.9203\n",
      "Epoch 11/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.2800 - acc: 0.9270\n",
      "Epoch 12/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.2659 - acc: 0.9306\n",
      "Epoch 13/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.2583 - acc: 0.9325\n",
      "Epoch 14/25\n",
      "48003/48003 [==============================] - 12s 258us/step - loss: 0.2489 - acc: 0.9357\n",
      "Epoch 15/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.2313 - acc: 0.9409\n",
      "Epoch 16/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.2258 - acc: 0.9433\n",
      "Epoch 17/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.2199 - acc: 0.9455\n",
      "Epoch 18/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.2129 - acc: 0.9461\n",
      "Epoch 19/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.2076 - acc: 0.9480\n",
      "Epoch 20/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.1978 - acc: 0.9510\n",
      "Epoch 21/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.1946 - acc: 0.9529\n",
      "Epoch 22/25\n",
      "48003/48003 [==============================] - 12s 258us/step - loss: 0.1932 - acc: 0.9520\n",
      "Epoch 23/25\n",
      "48003/48003 [==============================] - 12s 256us/step - loss: 0.1860 - acc: 0.9550\n",
      "Epoch 24/25\n",
      "48003/48003 [==============================] - 12s 259us/step - loss: 0.1872 - acc: 0.9539\n",
      "Epoch 25/25\n",
      "48003/48003 [==============================] - 12s 257us/step - loss: 0.1795 - acc: 0.9557\n",
      "24001/24001 [==============================] - 4s 156us/step\n",
      "Epoch 1/25\n",
      "48002/48002 [==============================] - 15s 322us/step - loss: 1.3811 - acc: 0.4299\n",
      "Epoch 2/25\n",
      "48002/48002 [==============================] - 12s 259us/step - loss: 0.9346 - acc: 0.6715\n",
      "Epoch 3/25\n",
      "48002/48002 [==============================] - 12s 260us/step - loss: 1.1014 - acc: 0.7359\n",
      "Epoch 4/25\n",
      "48002/48002 [==============================] - 13s 260us/step - loss: 2.0099 - acc: 0.7567\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48002/48002 [==============================] - 12s 259us/step - loss: 2.5826 - acc: 0.7583\n",
      "Epoch 6/25\n",
      "48002/48002 [==============================] - 13s 270us/step - loss: 2.8427 - acc: 0.7578\n",
      "Epoch 7/25\n",
      "48002/48002 [==============================] - 13s 273us/step - loss: 3.0708 - acc: 0.7534\n",
      "Epoch 8/25\n",
      "48002/48002 [==============================] - 13s 261us/step - loss: 3.3869 - acc: 0.7379\n",
      "Epoch 9/25\n",
      "48002/48002 [==============================] - 12s 259us/step - loss: 3.7966 - acc: 0.7174\n",
      "Epoch 10/25\n",
      "48002/48002 [==============================] - 12s 257us/step - loss: 4.4214 - acc: 0.6847\n",
      "Epoch 11/25\n",
      "48002/48002 [==============================] - 12s 260us/step - loss: 4.6126 - acc: 0.6785\n",
      "Epoch 12/25\n",
      "48002/48002 [==============================] - 12s 257us/step - loss: 4.8746 - acc: 0.6766\n",
      "Epoch 13/25\n",
      "48002/48002 [==============================] - 12s 260us/step - loss: 4.8255 - acc: 0.6832\n",
      "Epoch 14/25\n",
      "48002/48002 [==============================] - 12s 259us/step - loss: 4.8470 - acc: 0.6835\n",
      "Epoch 15/25\n",
      "48002/48002 [==============================] - 12s 257us/step - loss: 4.8212 - acc: 0.6862\n",
      "Epoch 16/25\n",
      "48002/48002 [==============================] - 12s 259us/step - loss: 4.6984 - acc: 0.6937\n",
      "Epoch 17/25\n",
      "48002/48002 [==============================] - 12s 258us/step - loss: 4.7606 - acc: 0.6912\n",
      "Epoch 18/25\n",
      "48002/48002 [==============================] - 12s 259us/step - loss: 4.8031 - acc: 0.6891\n",
      "Epoch 19/25\n",
      "48002/48002 [==============================] - 12s 259us/step - loss: 4.9375 - acc: 0.6815\n",
      "Epoch 20/25\n",
      "48002/48002 [==============================] - 12s 258us/step - loss: 4.8300 - acc: 0.6863\n",
      "Epoch 21/25\n",
      "48002/48002 [==============================] - 12s 259us/step - loss: 4.8555 - acc: 0.6850\n",
      "Epoch 22/25\n",
      "48002/48002 [==============================] - 12s 260us/step - loss: 4.9450 - acc: 0.6813\n",
      "Epoch 23/25\n",
      "48002/48002 [==============================] - 12s 258us/step - loss: 4.9300 - acc: 0.6823\n",
      "Epoch 24/25\n",
      "48002/48002 [==============================] - 12s 259us/step - loss: 4.8481 - acc: 0.6876\n",
      "Epoch 25/25\n",
      "48002/48002 [==============================] - 12s 258us/step - loss: 4.8975 - acc: 0.6839\n",
      "24002/24002 [==============================] - 4s 159us/step\n",
      "Epoch 1/25\n",
      "48003/48003 [==============================] - 16s 325us/step - loss: 1.8660 - acc: 0.4634\n",
      "Epoch 2/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 5.1294 - acc: 0.5913\n",
      "Epoch 3/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 5.9751 - acc: 0.5963\n",
      "Epoch 4/25\n",
      "48003/48003 [==============================] - 12s 260us/step - loss: 6.0536 - acc: 0.5934\n",
      "Epoch 5/25\n",
      "48003/48003 [==============================] - 13s 271us/step - loss: 6.0469 - acc: 0.5965\n",
      "Epoch 6/25\n",
      "48003/48003 [==============================] - 14s 282us/step - loss: 6.1765 - acc: 0.5886\n",
      "Epoch 7/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 6.4028 - acc: 0.5775\n",
      "Epoch 8/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 6.5937 - acc: 0.5666\n",
      "Epoch 9/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 6.6582 - acc: 0.5628\n",
      "Epoch 10/25\n",
      "48003/48003 [==============================] - 12s 260us/step - loss: 8.0831 - acc: 0.4770\n",
      "Epoch 11/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 8.6890 - acc: 0.4394\n",
      "Epoch 12/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 9.8143 - acc: 0.3696\n",
      "Epoch 13/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 9.8296 - acc: 0.3691\n",
      "Epoch 14/25\n",
      "48003/48003 [==============================] - 12s 259us/step - loss: 10.0276 - acc: 0.3583\n",
      "Epoch 15/25\n",
      "48003/48003 [==============================] - 12s 258us/step - loss: 11.0128 - acc: 0.2982\n",
      "Epoch 16/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 11.2648 - acc: 0.2816\n",
      "Epoch 17/25\n",
      "48003/48003 [==============================] - 12s 260us/step - loss: 11.2721 - acc: 0.2828\n",
      "Epoch 18/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 11.4183 - acc: 0.2730\n",
      "Epoch 19/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 11.3071 - acc: 0.2807\n",
      "Epoch 20/25\n",
      "48003/48003 [==============================] - 12s 259us/step - loss: 11.4117 - acc: 0.2762\n",
      "Epoch 21/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 11.4667 - acc: 0.2735\n",
      "Epoch 22/25\n",
      "48003/48003 [==============================] - 12s 260us/step - loss: 11.5394 - acc: 0.2704\n",
      "Epoch 23/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 11.5894 - acc: 0.2675\n",
      "Epoch 24/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 11.5764 - acc: 0.2695\n",
      "Epoch 25/25\n",
      "48003/48003 [==============================] - 13s 260us/step - loss: 12.3639 - acc: 0.2213\n",
      "24001/24001 [==============================] - 4s 163us/step\n",
      "Epoch 1/25\n",
      "48003/48003 [==============================] - 16s 328us/step - loss: 1.0894 - acc: 0.5393\n",
      "Epoch 2/25\n",
      "48003/48003 [==============================] - 13s 274us/step - loss: 2.7474 - acc: 0.6446\n",
      "Epoch 3/25\n",
      "48003/48003 [==============================] - 13s 268us/step - loss: 5.3098 - acc: 0.6250\n",
      "Epoch 4/25\n",
      "48003/48003 [==============================] - 13s 279us/step - loss: 6.1655 - acc: 0.5855\n",
      "Epoch 5/25\n",
      "48003/48003 [==============================] - 14s 283us/step - loss: 7.5693 - acc: 0.5115\n",
      "Epoch 6/25\n",
      "48003/48003 [==============================] - 13s 264us/step - loss: 7.7025 - acc: 0.5067\n",
      "Epoch 7/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 7.5859 - acc: 0.5164\n",
      "Epoch 8/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 8.2912 - acc: 0.4736\n",
      "Epoch 9/25\n",
      "48003/48003 [==============================] - 13s 263us/step - loss: 9.2028 - acc: 0.4186\n",
      "Epoch 10/25\n",
      "48003/48003 [==============================] - 13s 263us/step - loss: 9.1863 - acc: 0.4195\n",
      "Epoch 11/25\n",
      "48003/48003 [==============================] - 12s 260us/step - loss: 9.1905 - acc: 0.4202\n",
      "Epoch 12/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 9.3693 - acc: 0.4098\n",
      "Epoch 13/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 10.9011 - acc: 0.3169\n",
      "Epoch 14/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 10.9502 - acc: 0.3159\n",
      "Epoch 15/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 10.9736 - acc: 0.3159\n",
      "Epoch 16/25\n",
      "48003/48003 [==============================] - 12s 260us/step - loss: 10.9740 - acc: 0.3170\n",
      "Epoch 17/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 11.0711 - acc: 0.3112\n",
      "Epoch 18/25\n",
      "48003/48003 [==============================] - 12s 260us/step - loss: 10.9931 - acc: 0.3168\n",
      "Epoch 19/25\n",
      "48003/48003 [==============================] - 13s 263us/step - loss: 11.1099 - acc: 0.3093\n",
      "Epoch 20/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 11.1834 - acc: 0.3052\n",
      "Epoch 21/25\n",
      "48003/48003 [==============================] - 12s 260us/step - loss: 11.1307 - acc: 0.3087\n",
      "Epoch 22/25\n",
      "48003/48003 [==============================] - 12s 260us/step - loss: 11.1834 - acc: 0.3053\n",
      "Epoch 23/25\n",
      "48003/48003 [==============================] - 13s 262us/step - loss: 11.1873 - acc: 0.3050\n",
      "Epoch 24/25\n",
      "48003/48003 [==============================] - 13s 261us/step - loss: 12.4124 - acc: 0.2296\n",
      "Epoch 25/25\n",
      "48003/48003 [==============================] - 13s 263us/step - loss: 12.5731 - acc: 0.2197\n",
      "24001/24001 [==============================] - 4s 169us/step\n",
      "Epoch 1/35\n",
      "48002/48002 [==============================] - 17s 346us/step - loss: 0.8243 - acc: 0.6325\n",
      "Epoch 2/35\n",
      "48002/48002 [==============================] - 13s 278us/step - loss: 0.4312 - acc: 0.8498\n",
      "Epoch 3/35\n",
      "48002/48002 [==============================] - 13s 277us/step - loss: 0.3463 - acc: 0.8864\n",
      "Epoch 4/35\n",
      "48002/48002 [==============================] - 14s 283us/step - loss: 0.3026 - acc: 0.9071\n",
      "Epoch 5/35\n",
      "48002/48002 [==============================] - 13s 269us/step - loss: 0.2789 - acc: 0.9127\n",
      "Epoch 6/35\n",
      "48002/48002 [==============================] - 13s 269us/step - loss: 0.2558 - acc: 0.9236\n",
      "Epoch 7/35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48002/48002 [==============================] - 13s 270us/step - loss: 0.2529 - acc: 0.9246\n",
      "Epoch 8/35\n",
      "48002/48002 [==============================] - 13s 267us/step - loss: 0.2371 - acc: 0.9294\n",
      "Epoch 9/35\n",
      "48002/48002 [==============================] - 13s 270us/step - loss: 0.2219 - acc: 0.9358\n",
      "Epoch 10/35\n",
      "48002/48002 [==============================] - 13s 269us/step - loss: 0.2182 - acc: 0.9374\n",
      "Epoch 11/35\n",
      "48002/48002 [==============================] - 13s 268us/step - loss: 0.2115 - acc: 0.9415\n",
      "Epoch 12/35\n",
      "48002/48002 [==============================] - 13s 271us/step - loss: 0.2028 - acc: 0.9438\n",
      "Epoch 13/35\n",
      "48002/48002 [==============================] - 13s 268us/step - loss: 0.2004 - acc: 0.9453\n",
      "Epoch 14/35\n",
      "48002/48002 [==============================] - 13s 268us/step - loss: 0.1933 - acc: 0.9467\n",
      "Epoch 15/35\n",
      "48002/48002 [==============================] - 13s 268us/step - loss: 0.1839 - acc: 0.9491\n",
      "Epoch 16/35\n",
      "48002/48002 [==============================] - 13s 269us/step - loss: 0.1843 - acc: 0.9499\n",
      "Epoch 17/35\n",
      "48002/48002 [==============================] - 13s 270us/step - loss: 0.1849 - acc: 0.9505\n",
      "Epoch 18/35\n",
      "48002/48002 [==============================] - 13s 269us/step - loss: 0.1823 - acc: 0.9509\n",
      "Epoch 19/35\n",
      "48002/48002 [==============================] - 13s 270us/step - loss: 0.1717 - acc: 0.9534\n",
      "Epoch 20/35\n",
      "48002/48002 [==============================] - 13s 268us/step - loss: 0.1814 - acc: 0.9532\n",
      "Epoch 21/35\n",
      "48002/48002 [==============================] - 13s 271us/step - loss: 0.1725 - acc: 0.9541\n",
      "Epoch 22/35\n",
      "48002/48002 [==============================] - 13s 269us/step - loss: 0.1691 - acc: 0.9557\n",
      "Epoch 23/35\n",
      "48002/48002 [==============================] - 13s 271us/step - loss: 0.1708 - acc: 0.9540\n",
      "Epoch 24/35\n",
      "48002/48002 [==============================] - 13s 271us/step - loss: 0.1673 - acc: 0.9548\n",
      "Epoch 25/35\n",
      "48002/48002 [==============================] - 3203s 67ms/step - loss: 0.1665 - acc: 0.9558\n",
      "Epoch 26/35\n",
      " 7712/48002 [===>..........................] - ETA: 3:33:25 - loss: 0.1651 - acc: 0.9532 ETA: 3:23:46 -  - ETA: 3:31:37 - loss: "
     ]
    }
   ],
   "source": [
    "for i in range(len(a)):\n",
    "  \n",
    "    j = a[i][0]\n",
    "    son = a[i][1].capitalize()\n",
    "    print(f\"j = {a[i][0]}\\nson = {a[i][1]}\")\n",
    "\n",
    "    directory = os.getcwd()\n",
    "    \n",
    "    if os.path.isdir(os.getcwd() + '/{}_{}'.format(j,son)) == True:\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(os.getcwd() + '/{}_{}'.format(j,son))\n",
    "    \n",
    "    t = TicToc()\n",
    "\n",
    "    if son=='S':\n",
    "        df1 = fixrows('Potencia_r1_00').iloc[:j,:]\n",
    "        num_row = np.shape(df1)[0]\n",
    "        coords  = ['(0,0)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df1 = coords.join(df1, how='left')\n",
    "        df1.to_csv('Potencia_R1_00.csv')\n",
    "         \n",
    "        df2 = fixrows('Potencia_r1_01').iloc[:j,:]\n",
    "        num_row = np.shape(df2)[0]\n",
    "        coords  = ['(0,1)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df2 = coords.join(df2, how='left')\n",
    "        df2.to_csv('Potencia_R1_01.csv')\n",
    "\n",
    "        df3 = fixrows('Potencia_r1_02').iloc[:j,:]\n",
    "        num_row = np.shape(df3)[0]\n",
    "        coords  = ['(0,2)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df3 = coords.join(df3, how='left')\n",
    "        df3.to_csv('Potencia_R1_02.csv')\n",
    "\n",
    "        df4 = fixrows('Potencia_r2_10').iloc[:j,:]\n",
    "        num_row = np.shape(df4)[0]\n",
    "        coords  = ['(1,0)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df4 = coords.join(df4, how='left')\n",
    "        df4.to_csv('Potencia_R2_10.csv')\n",
    "\n",
    "        df5 = fixrows('Potencia_r2_11').iloc[:j,:]\n",
    "        num_row = np.shape(df5)[0]\n",
    "        coords  = ['(1,1)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df5 = coords.join(df5, how='left')\n",
    "        df5.to_csv('Potencia_R2_11.csv')\n",
    "\n",
    "        df6 = fixrows('Potencia_r2_12').iloc[:j,:]\n",
    "        num_row = np.shape(df6)[0]\n",
    "        coords  = ['(1,2)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df6 = coords.join(df6, how='left')\n",
    "        df6.to_csv('Potencia_R2_12.csv')\n",
    "\n",
    "        df7 = fixrows('Potencia_r3_20').iloc[:j,:]\n",
    "        num_row = np.shape(df7)[0]\n",
    "        coords  = ['(2,0)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df7 = coords.join(df7, how='left')\n",
    "        df7.to_csv('Potencia_R3_20.csv')\n",
    "\n",
    "        df8 = fixrows('Potencia_r3_21').iloc[:j,:]\n",
    "        num_row = np.shape(df8)[0]\n",
    "        coords  = ['(2,1)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df8 = coords.join(df8, how='left')\n",
    "        df8.to_csv('Potencia_R3_21.csv')\n",
    "\n",
    "        df9 = fixrows('Potencia_r3_22').iloc[:j,:]\n",
    "        num_row = np.shape(df9)[0]\n",
    "        coords  = ['(2,2)' for j in range(num_row)]\n",
    "        coords = pd.DataFrame(coords,dtype=object, columns = ['X,Y'])\n",
    "        df9 = coords.join(df9, how='left')\n",
    "        df9.to_csv('Potencia_R3_22.csv')\n",
    "          \n",
    "        # Fusionar archivos corregidos para obtener el archivo de potencias final\n",
    "        fusionar_csv('Potencia_R1_00','Potencia_R1_01','Potencia_R1_02','Potencia_R2_10','Potencia_R2_11',\n",
    "                 'Potencia_R2_12','Potencia_R3_20','Potencia_R3_21','Potencia_R3_22')\n",
    "         \n",
    "        df0 = fixrows('potencias_fusionado').iloc[:,1:]\n",
    "        os.system('rm Potencia_R*')\n",
    "        os.system('rm Potencia_r*_corregido.csv')\n",
    "        df0 = pd.read_csv('potencias_fusionado_corregido.csv').iloc[3:,1:]\n",
    "        os.system('play -nq -t alsa synth {} sine {}'.format(duration,f1))\n",
    "    else:\n",
    "        df0 = pd.read_csv('potencias_fusionado_corregido.csv').iloc[3:,1:]\n",
    "        os.system('play -nq -t alsa synth {} sine {}'.format(duration,f1))\n",
    "\n",
    "    X = df0.iloc[:,1:].values #variables Dependientes (Potencias)\n",
    "    y = df0.iloc[:,0].values #values Independientes (Posici√≥n)\n",
    "\n",
    "    from keras.utils import np_utils\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    y_encoded = encoder.transform(y)\n",
    "    y_encoded = np_utils.to_categorical(y_encoded)\n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    # =============================================================================\n",
    "    # Feature Scaling (Standarization)\n",
    "    # =============================================================================\n",
    "    if son == 'S':\n",
    "        print(son)\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "    else:\n",
    "        print(son)\n",
    "        # Feature Scaling (Normalization)\n",
    "        from sklearn import preprocessing\n",
    "        X_train = preprocessing.normalize(X_train)\n",
    "        X_test  = preprocessing.normalize(X_test)\n",
    "    # =============================================================================\n",
    "    # Buscar mejores par√°metros\n",
    "    # =============================================================================\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    print(\"Comenzando Grid_search\\n\")\n",
    "    t.tic()\n",
    "\n",
    "    def build_classifier(optimizer):\n",
    "        classifier = Sequential()\n",
    "        classifier.add(Dense(units = np.shape(X_test)[1]+1, kernel_initializer = 'uniform', activation = 'relu', input_dim = np.shape(X_test)[1]))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "        \n",
    "        classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "        \n",
    "        classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))    \n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "        classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "        classifier.add(Dense(units = np.shape(y_test)[1], kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "        classifier.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        return classifier\n",
    "\n",
    "    classifier = KerasClassifier(build_fn = build_classifier)\n",
    "    parameters = {'batch_size': [16,32,48],'epochs': [15, 25, 35],'optimizer': ['adam', 'adamax','rmsprop']}\n",
    "\n",
    "    grid = GridSearchCV(estimator = classifier,param_grid = parameters,\n",
    "    #                           scoring = 'accuracy',\n",
    "                        cv = 3,n_jobs=-3)\n",
    "\n",
    "    grid_search_results = grid.fit(X_train, y_train)\n",
    "\n",
    "    best_parameters  = grid_search_results.best_params_\n",
    "    print(f\"best_parameters = {grid_search_results.best_params_}\")\n",
    "    print(f\"best_accuracy =   {grid_search_results.best_score_}\")\n",
    "    t.toc('Finalizado - Grid_search')\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration,f2))\n",
    "    t1 = t.tocvalue()\n",
    "\n",
    "    means = grid_search_results.cv_results_['mean_test_score']\n",
    "    stds = grid_search_results.cv_results_['std_test_score']\n",
    "    params = grid_search_results.cv_results_['params']\n",
    "\n",
    "    # =============================================================================\n",
    "    # Cross Validation\n",
    "    # =============================================================================\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    t.tic() \n",
    "    print(\"\\nEntrando en Cross Validation\\n\")\n",
    "\n",
    "    def build_classifier():\n",
    "        classifier = Sequential()\n",
    "        classifier.add(Dense(units = np.shape(X_test)[1]+1, kernel_initializer = 'uniform', activation = 'relu', input_dim = np.shape(X_test)[1]))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "        \n",
    "        classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "        \n",
    "        classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))    \n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "        classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "        classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "        classifier.add(Dense(units = np.shape(y_test)[1], kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "        classifier.compile(optimizer = best_parameters['optimizer'], loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        return classifier\n",
    "\n",
    "    classifier = KerasClassifier(build_fn = build_classifier, batch_size = best_parameters['batch_size'], epochs = best_parameters['epochs'])\n",
    "\n",
    "    accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5, n_jobs = -1)\n",
    "    ac = list(accuracies)\n",
    "    mean = accuracies.mean()\n",
    "    variance = accuracies.std()\n",
    "    t.toc('\\nTiempo Cross-Validation: ')\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration,f3))\n",
    "    time = t.tocvalue()\n",
    "\n",
    "    # =============================================================================\n",
    "    #     Distribuci√≥n de probabilidad\n",
    "    # =============================================================================\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "        \n",
    "    print('>> Mean CV score is: ', round(np.mean(accuracies),3))\n",
    "    pltt = sns.distplot(pd.Series(accuracies,name='CV scores distribution'), color='r')\n",
    "    plt.savefig('CV_Accuracies_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    # =====================================================\n",
    "    #     Saving model\n",
    "    # =============================================================================\n",
    "\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = np.shape(X_test)[1]+1, kernel_initializer = 'uniform', activation = 'relu', input_dim = np.shape(X_test)[1]))\n",
    "    classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "    classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "    classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))    \n",
    "    classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "    classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))    \n",
    "    classifier.add(Dropout(rate = 0.2))\n",
    "\n",
    "    classifier.add(Dense(units = np.shape(y_test)[1], kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "    classifier.compile(optimizer = best_parameters['optimizer'], loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = classifier.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # save model and architecture to single file\n",
    "    classifier.save(directory + \"/{}_{}/model_{}.h5\".format(j,son,j))\n",
    "    print(\"Saved model to disk\\n\")\n",
    "\n",
    "    # =============================================================================\n",
    "    #     Escritura de archivo\n",
    "    # =============================================================================\n",
    "    print(f\"\\nnp.shape(X_test)[0] = {np.shape(X_test)[0]}\\n\")\n",
    "\n",
    "    outFileName= directory + \"/{}_{}/resultados_{}.txt\".format(j,son,j)\n",
    "    f = open(outFileName,\"w\")\n",
    "\n",
    "    f.write(\"El n√∫mero de elementos usados es: {}\\n\".format(j)+\n",
    "        \"Los mejores par√°metros son: {}\".format(best_parameters) +\n",
    "        \"\\nTiempo de GridSearchCV  = {}\".format(round(int(t1/60),2)) + \n",
    "        \"\\nTiempo red neuronal  =  {}\".format(round(int(time/60),2)) + \n",
    "        \"\\nLa media obtenida es: {}\".format(mean)+\n",
    "        \"\\nLa varianza obtenida es: {}\".format(variance) + '\\n'\n",
    "        )\n",
    "    for i in ac:\n",
    "        f.write(\"\\tac: \" + repr(round((i*100),2)) +\"%\" + '\\n')\n",
    "\n",
    "    for i,j in enumerate(zip(params,means,stds)):\n",
    "        f.write(\"\\nparams[{}] = {} --> means[{}] = {}\\n\".format(i,params[i],i,round(means[i],2)))\n",
    "    # =============================================================================\n",
    "    #                             Prediction\n",
    "    # =============================================================================\n",
    "    for i in range(1,15):\n",
    "        r = random.randint(0, np.shape(X_test)[0]-1)\n",
    "        y_pred = classifier.predict(np.array([X_test[r]]))\n",
    "        predictions = list(encoder.inverse_transform([np.argmax(y_pred, axis=None, out=None)]))\n",
    "        y_pred_prob = classifier.predict_proba(np.array([X_test[r]]))\n",
    "        f.write(\"\\nFor the vector: [\"+ repr(X_test[r])+ \"]\\t the predicted position is:\" +  repr(predictions) +  \"and its accuracy was:\" + repr(round(np.amax(y_pred_prob),2)))\n",
    "    f.close()\n",
    "\n",
    "    print(\"Archivo escrito\\n\")\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration,f1))\n",
    "    # =============================================================================\n",
    "    # Full multiclass report \n",
    "    # =============================================================================\n",
    "\n",
    "    from plot_history import * \n",
    "    from full_multiclass_report import * \n",
    "\n",
    "    history = classifier.fit(X_train, \n",
    "                    y_train,\n",
    "                    epochs = best_parameters['epochs'],\n",
    "                    batch_size = best_parameters['batch_size'],\n",
    "                    verbose=0,\n",
    "                    validation_data=(X_test,y_test))\n",
    "    plot_history(history)\n",
    "\n",
    "    full_multiclass_report(classifier,X_test,y_test,classes=['(0,0)','(0,1)','(0,2)','(1,0)','(1,1)','(1,2)','(2,0)','(2,1)','(2,2)'])\n",
    "\n",
    "    # =============================================================================\n",
    "    #     Move file to folder\n",
    "    # =============================================================================\n",
    "    mv = directory + '/{}_{}/'.format(j,son)\n",
    "    os.system('mv Confusion_matrix.png '+ mv+ 'Confusion_matrix.png')\n",
    "    os.system('mv CV_Accuracies_distribution.png '+ mv+ 'CV_Accuracies_distribution.png')\n",
    "    os.system('mv Loss.png '+ mv + 'Loss.png')\n",
    "    os.system('mv Accuracy.png '+ mv + 'Accuracy.png')\n",
    "    os.system('mv Classification_report.csv ' + mv + 'Classification_report.csv')\n",
    "    os.system('mv model.json ' + mv + 'model_{}_{}.json'.format(j,son))\n",
    "    os.system('play -nq -t alsa synth {} sine {}'.format(duration,f4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
